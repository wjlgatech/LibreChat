<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice AI - Auto Working Solution</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            margin-bottom: 10px;
        }
        .info {
            background-color: #e3f2fd;
            border: 1px solid #90caf9;
            color: #1565c0;
            padding: 15px;
            border-radius: 4px;
            margin-bottom: 20px;
        }
        .controls {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
            flex-wrap: wrap;
        }
        button {
            padding: 10px 20px;
            border: none;
            border-radius: 4px;
            font-size: 16px;
            cursor: pointer;
        }
        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        .start-btn {
            background-color: #4CAF50;
            color: white;
        }
        .stop-btn {
            background-color: #f44336;
            color: white;
        }
        .conversation {
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 20px;
            min-height: 300px;
            max-height: 500px;
            overflow-y: auto;
            background-color: #fafafa;
        }
        .message {
            margin-bottom: 15px;
            padding: 10px;
            border-radius: 8px;
        }
        .user-message {
            background-color: #e3f2fd;
            margin-left: 20%;
            text-align: right;
        }
        .ai-message {
            background-color: #f5f5f5;
            margin-right: 20%;
        }
        .status {
            padding: 10px;
            background-color: #f0f0f0;
            border-radius: 4px;
            margin-bottom: 20px;
            font-weight: bold;
        }
        .api-config {
            margin-bottom: 20px;
            padding: 15px;
            background-color: #f9f9f9;
            border-radius: 4px;
        }
        input {
            padding: 8px;
            margin-right: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            width: 300px;
        }
        .vad-indicator {
            display: inline-block;
            width: 200px;
            height: 20px;
            background-color: #e0e0e0;
            border-radius: 10px;
            margin-left: 10px;
            overflow: hidden;
        }
        .vad-level {
            height: 100%;
            background-color: #4CAF50;
            transition: width 0.1s;
        }
        .debug {
            font-family: monospace;
            font-size: 12px;
            background-color: #f5f5f5;
            padding: 10px;
            border-radius: 4px;
            max-height: 150px;
            overflow-y: auto;
            margin-top: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Voice AI - Auto Working Solution</h1>
        
        <div class="info">
            ðŸŽ¯ <strong>Auto-stops when you stop speaking</strong>
            <br>ðŸ”„ Continuous conversation without manual clicks
            <br>âš¡ Visual VAD indicator shows voice detection
        </div>

        <div class="api-config">
            <label for="apiKey">OpenAI API Key:</label>
            <input type="password" id="apiKey" placeholder="sk-...">
            <button onclick="saveApiKey()" class="start-btn">Save Key</button>
            <span id="apiKeyStatus"></span>
        </div>

        <div class="status">
            Status: <span id="status">Ready</span>
            <span class="vad-indicator">
                <div class="vad-level" id="vadLevel" style="width: 0%"></div>
            </span>
            <br>
            <small>RMS: <span id="rmsValue">0.000</span> | Threshold: <span id="thresholdValue">0.015</span></small>
        </div>

        <div class="controls">
            <button id="startBtn" onclick="start()" class="start-btn" disabled>Start Conversation</button>
            <button id="stopBtn" onclick="stop()" class="stop-btn" disabled>Stop</button>
            <button onclick="clearChat()">Clear</button>
            <button onclick="adjustSensitivity('increase')">+ Sensitivity</button>
            <button onclick="adjustSensitivity('decrease')">- Sensitivity</button>
            <button onclick="adjustTiming('faster')">Faster</button>
            <button onclick="adjustTiming('slower')">Slower</button>
        </div>

        <div class="conversation" id="conversation">
            <div style="text-align: center; color: #999; padding: 50px;">
                Enter your API key and click "Start Conversation" to begin
            </div>
        </div>

        <div class="debug" id="debug"></div>
    </div>

    <script>
        let apiKey = '';
        let mediaRecorder = null;
        let audioChunks = [];
        let stream = null;
        let isRecording = false;
        let conversationHistory = [];
        
        // VAD variables
        let audioContext = null;
        let analyser = null;
        let dataArray = null;
        let isSpeaking = false;
        let silenceStartTime = null;
        let hasSpoken = false;
        
        // Settings
        let SILENCE_THRESHOLD = 0.040;  // Sweet spot for most users
        let SILENCE_DURATION = 500;     // Half second silence for natural conversation flow
        const MIN_SPEECH_DURATION = 300;  // Quick speech detection
        
        function log(msg) {
            console.log(msg);
            const debug = document.getElementById('debug');
            debug.innerHTML += `${new Date().toLocaleTimeString()}: ${msg}<br>`;
            debug.scrollTop = debug.scrollHeight;
        }
        
        function setStatus(status) {
            document.getElementById('status').textContent = status;
            log(status);
        }
        
        function saveApiKey() {
            apiKey = document.getElementById('apiKey').value.trim();
            if (apiKey && apiKey.startsWith('sk-')) {
                localStorage.setItem('openai_api_key', apiKey);
                document.getElementById('apiKeyStatus').textContent = 'âœ“ Saved';
                document.getElementById('startBtn').disabled = false;
            } else {
                alert('Invalid API key');
            }
        }
        
        async function start() {
            try {
                setStatus('Getting microphone access...');
                stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                // Setup audio context for VAD
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                const source = audioContext.createMediaStreamSource(stream);
                source.connect(analyser);
                
                analyser.fftSize = 2048;
                const bufferLength = analyser.frequencyBinCount;
                dataArray = new Uint8Array(bufferLength);
                
                // Setup media recorder
                mediaRecorder = new MediaRecorder(stream);
                mediaRecorder.ondataavailable = (e) => {
                    if (e.data.size > 0) {
                        audioChunks.push(e.data);
                    }
                };
                
                mediaRecorder.onstop = processRecording;
                
                document.getElementById('startBtn').disabled = true;
                document.getElementById('stopBtn').disabled = false;
                
                startRecording();
                
            } catch (error) {
                alert('Microphone access denied');
                console.error(error);
            }
        }
        
        function stop() {
            isRecording = false;
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
            }
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
            }
            if (audioContext) {
                audioContext.close();
            }
            
            document.getElementById('startBtn').disabled = false;
            document.getElementById('stopBtn').disabled = true;
            setStatus('Stopped');
        }
        
        function startRecording() {
            audioChunks = [];
            hasSpoken = false;
            isSpeaking = false;
            silenceStartTime = null;
            
            mediaRecorder.start();
            isRecording = true;
            setStatus('Listening... (speak now)');
            
            // Start VAD monitoring
            monitorAudio();
        }
        
        function monitorAudio() {
            if (!isRecording) return;
            
            analyser.getByteFrequencyData(dataArray);
            
            // Calculate RMS
            let sum = 0;
            for (let i = 0; i < dataArray.length; i++) {
                sum += dataArray[i] * dataArray[i];
            }
            const rms = Math.sqrt(sum / dataArray.length) / 255;
            
            // Update visual indicator
            document.getElementById('vadLevel').style.width = (rms * 100 * 5) + '%';
            document.getElementById('rmsValue').textContent = rms.toFixed(3);
            
            const now = Date.now();
            
            if (rms > SILENCE_THRESHOLD) {
                // Speech detected
                if (!isSpeaking) {
                    isSpeaking = true;
                    hasSpoken = true;
                    silenceStartTime = null;
                    log(`Speech started (RMS: ${rms.toFixed(3)})`);
                }
            } else {
                // Silence detected
                if (isSpeaking) {
                    isSpeaking = false;
                    silenceStartTime = now;
                    log('Speech ended, silence started');
                } else if (hasSpoken && silenceStartTime) {
                    // Check if silence duration exceeded
                    if (now - silenceStartTime > SILENCE_DURATION) {
                        log('Silence threshold reached, stopping recording');
                        isRecording = false;
                        mediaRecorder.stop();
                        return;
                    }
                }
            }
            
            // Continue monitoring
            requestAnimationFrame(monitorAudio);
        }
        
        async function processRecording() {
            setStatus('Processing...');
            
            if (audioChunks.length === 0) {
                log('No audio recorded');
                setTimeout(startRecording, 500);
                return;
            }
            
            const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
            log(`Audio size: ${audioBlob.size} bytes`);
            
            if (audioBlob.size < 5000) {
                log('Audio too small, skipping');
                setTimeout(startRecording, 500);
                return;
            }
            
            try {
                // Transcribe
                const transcript = await transcribe(audioBlob);
                if (!transcript) {
                    setTimeout(startRecording, 500);
                    return;
                }
                
                addMessage(transcript, 'user');
                
                // Get AI response
                const response = await getResponse(transcript);
                addMessage(response, 'ai');
                
                // Play TTS
                await playTTS(response);
                
                // Continue conversation
                setTimeout(startRecording, 500);
                
            } catch (error) {
                log(`Error: ${error.message}`);
                setTimeout(startRecording, 1000);
            }
        }
        
        async function transcribe(audioBlob) {
            const formData = new FormData();
            formData.append('file', audioBlob, 'audio.webm');
            formData.append('model', 'whisper-1');
            
            const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${apiKey}`
                },
                body: formData
            });
            
            if (!response.ok) {
                throw new Error('Transcription failed');
            }
            
            const data = await response.json();
            log(`Transcribed: "${data.text}"`);
            return data.text;
        }
        
        async function getResponse(text) {
            const messages = [
                { role: 'system', content: 'You are a helpful assistant. Keep responses brief and conversational.' },
                ...conversationHistory.slice(-4),
                { role: 'user', content: text }
            ];
            
            const response = await fetch('https://api.openai.com/v1/chat/completions', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${apiKey}`
                },
                body: JSON.stringify({
                    model: 'gpt-3.5-turbo',
                    messages: messages,
                    max_tokens: 100
                })
            });
            
            if (!response.ok) {
                throw new Error('Chat failed');
            }
            
            const data = await response.json();
            return data.choices[0].message.content;
        }
        
        async function playTTS(text) {
            setStatus('Speaking...');
            
            const response = await fetch('https://api.openai.com/v1/audio/speech', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${apiKey}`
                },
                body: JSON.stringify({
                    model: 'tts-1',
                    input: text,
                    voice: 'alloy'
                })
            });
            
            if (!response.ok) {
                throw new Error('TTS failed');
            }
            
            const audioBlob = await response.blob();
            const audio = new Audio(URL.createObjectURL(audioBlob));
            
            return new Promise((resolve) => {
                audio.onended = resolve;
                audio.play();
            });
        }
        
        function addMessage(text, type) {
            const conv = document.getElementById('conversation');
            if (conv.querySelector('[style*="text-align: center"]')) {
                conv.innerHTML = '';
            }
            
            const msg = document.createElement('div');
            msg.className = 'message ' + (type === 'user' ? 'user-message' : 'ai-message');
            msg.textContent = text;
            conv.appendChild(msg);
            conv.scrollTop = conv.scrollHeight;
            
            conversationHistory.push({ role: type === 'user' ? 'user' : 'assistant', content: text });
        }
        
        function clearChat() {
            conversationHistory = [];
            document.getElementById('conversation').innerHTML = '<div style="text-align: center; color: #999; padding: 50px;">Cleared</div>';
            document.getElementById('debug').innerHTML = '';
        }
        
        // Adjustment functions
        function adjustSensitivity(direction) {
            if (direction === 'increase') {
                SILENCE_THRESHOLD = Math.max(0.005, SILENCE_THRESHOLD - 0.005);
            } else {
                SILENCE_THRESHOLD = Math.min(0.1, SILENCE_THRESHOLD + 0.005);
            }
            document.getElementById('thresholdValue').textContent = SILENCE_THRESHOLD.toFixed(3);
            log(`Threshold adjusted to ${SILENCE_THRESHOLD.toFixed(3)}`);
        }
        
        function adjustTiming(speed) {
            if (speed === 'faster') {
                SILENCE_DURATION = Math.max(300, SILENCE_DURATION - 200);
            } else {
                SILENCE_DURATION = Math.min(3000, SILENCE_DURATION + 200);
            }
            log(`Silence duration adjusted to ${SILENCE_DURATION}ms`);
        }
        
        // Load API key on start
        window.onload = () => {
            const saved = localStorage.getItem('openai_api_key');
            if (saved) {
                apiKey = saved;
                document.getElementById('apiKey').value = saved;
                document.getElementById('apiKeyStatus').textContent = 'âœ“ Loaded';
                document.getElementById('startBtn').disabled = false;
            }
            document.getElementById('thresholdValue').textContent = SILENCE_THRESHOLD.toFixed(3);
        };
    </script>
</body>
</html>